import streamlit as st
import os
import time
import torch
import random
import warnings
from langchain.schema import Document
from langchain_community.vectorstores import Chroma, FAISS
from langchain_openai import OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain_openai import ChatOpenAI
from langchain.chains import RetrievalQA, ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory  # âœ… ì˜¬ë°”ë¥¸ import ê²½ë¡œ
from langchain.prompts import PromptTemplate, ChatPromptTemplate
from langchain.schema import SystemMessage, HumanMessage, AIMessage
from langchain.llms.base import LLM
from langchain_core.runnables import RunnablePassthrough, RunnableParallel  # âœ… ìƒˆë¡œìš´ ì²´ì¸ ë°©ì‹
from typing import Any, List, Optional
from transformers import (
    AutoModelForCausalLM, 
    AutoTokenizer,
    BitsAndBytesConfig
)
import os
from pathlib import Path
import PyPDF2
from io import BytesIO

# âœ… ê²½ê³  ë©”ì‹œì§€ í•„í„°ë§
warnings.filterwarnings("ignore", category=UserWarning, module="requests")
warnings.filterwarnings("ignore", category=UserWarning, module="langchain")
warnings.filterwarnings("ignore", message=".*LangChainDeprecationWarning.*")

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="Tokbom - RAG Chatbot", layout="wide", page_icon="ğŸŒ¸")

# ì›¹ì•± ì œëª©
st.markdown("""
    <h1 style='text-align: center; color: #4CAF50; font-size: 3.5rem; margin-bottom: 0.5rem;'>
        Tokbom "RAGë¡œ í†¡! BeomAIì™€ í•¨ê»˜ ë§Œë“¤ì–´ ë³´ì„¸ìš”!" ğŸŒ¸
    </h1>
    """, unsafe_allow_html=True)

# CSS ìŠ¤íƒ€ì¼
st.markdown("""
<style>
.stApp {
    font-size: 1.2rem;
}

h1 {
    font-size: 2.5rem !important;
}

h2 {
    font-size: 2rem !important;
}

h3 {
    font-size: 1.5rem !important;
}

button[data-baseweb="button"] > div {
    font-size: 1.2rem !important;
}

.stSelectbox label, .stSlider label, .stTextArea label, .stTextInput label {
    font-size: 1.3rem !important;
}

.stSelectbox > div > div {
    font-size: 1.2rem !important;
}

button[data-baseweb="tab"] {
    font-size: 2.0rem !important;
    font-weight: bold !important;
    padding: 0.5rem 1rem !important;
    height: 48px !important;
    color: #1B5E20 !important;
}

.stChatMessage {
    font-size: 1.2rem !important;
}

.sidebar .stTextInput label {
    font-size: 1.3rem !important;
}

input {
    font-size: 1.2rem !important;
}
</style>
""", unsafe_allow_html=True)


# âœ… ìˆ˜ì •ëœ ìµœì í™”ëœ LLaMA í´ë˜ìŠ¤
class OptimizedDirectLLaMA(LLM):
    """ì–‘ìí™” ë° ìµœì í™”ê°€ ì ìš©ëœ LLaMA ëª¨ë¸ì„ ì§ì ‘ ì œì–´í•˜ëŠ” LangChain LLM ë˜í¼"""
    
    model: Any = None
    tokenizer: Any = None
    max_new_tokens: int = 256
    temperature: float = 0.7
    top_p: float = 0.95
    device: str = "cuda"
    use_cache: bool = True
    
    def _call(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
    ) -> str:
        """Run the LLM on the given prompt and input."""
        # ì…ë ¥ ê¸¸ì´ ì œí•œ
        max_input_length = 512 if self.device == "cpu" else 1024
        
        # âœ… attention_mask ìƒì„±ìœ¼ë¡œ ê²½ê³  í•´ê²°
        inputs = self.tokenizer(
            prompt, 
            return_tensors="pt", 
            truncation=True, 
            max_length=max_input_length,
            padding=True,
            add_special_tokens=True
        )
        
        # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
        input_ids = inputs['input_ids'].to(self.device)
        attention_mask = inputs['attention_mask'].to(self.device)
        
        # ì…ë ¥ì´ ë„ˆë¬´ ê¸¸ë©´ ê²½ê³ 
        if input_ids.shape[1] > 400:
            st.warning("ì…ë ¥ì´ ê¸¸ì–´ ì‘ë‹µì´ ëŠë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
        # ëª¨ë¸ ì¶”ë¡ 
        with torch.no_grad():
            if self.temperature == 0.0:
                outputs = self.model.generate(
                    input_ids,
                    attention_mask=attention_mask,  # âœ… attention_mask ì¶”ê°€
                    max_new_tokens=self.max_new_tokens,
                    do_sample=False,
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    use_cache=self.use_cache,
                    num_beams=1,
                    repetition_penalty=1.1,
                )
            else:
                outputs = self.model.generate(
                    input_ids,
                    attention_mask=attention_mask,  # âœ… attention_mask ì¶”ê°€
                    max_new_tokens=self.max_new_tokens,
                    temperature=self.temperature,
                    top_p=self.top_p,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    use_cache=self.use_cache,
                    num_beams=1,
                    repetition_penalty=1.1,
                )
        
        # ìƒì„±ëœ í…ìŠ¤íŠ¸ ë””ì½”ë”©
        generated_text = self.tokenizer.decode(
            outputs[0][input_ids.shape[1]:], 
            skip_special_tokens=True
        )
        
        # stop ë‹¨ì–´ ì²˜ë¦¬
        if stop:
            for stop_word in stop:
                if stop_word in generated_text:
                    generated_text = generated_text.split(stop_word)[0]
        
        return generated_text.strip()
    
    @property
    def _llm_type(self) -> str:
        return "optimized_direct_llama"


# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []
    
if "memory" not in st.session_state:
    # âœ… ìƒˆë¡œìš´ ë©”ëª¨ë¦¬ ë°©ì‹ ì‚¬ìš©
    st.session_state.memory = ConversationBufferMemory(
        memory_key="chat_history", 
        return_messages=True
    )
    
if "documents" not in st.session_state:
    st.session_state.documents = []
    
if "vectordb" not in st.session_state:
    st.session_state.vectordb = None
    
if "retriever" not in st.session_state:
    st.session_state.retriever = None
    
if "system_prompt" not in st.session_state:
    st.session_state.system_prompt = """ë‹¹ì‹ ì€ ì „ë¬¸ì ì¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì—…ë¡œë“œëœ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.

ë‹¤ìŒê³¼ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:
1. ë¬¸ì„œì˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•œ ì •ë³´ ì œê³µ
2. ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ ì„¤ëª…
3. í•„ìš”ì‹œ ë‹¨ê³„ë³„ ê°€ì´ë“œ ì œê³µ
"""
    
if "user_prompt_template" not in st.session_state:
    st.session_state.user_prompt_template = """
ì»¨í…ìŠ¤íŠ¸: {context}

ëŒ€í™” ê¸°ë¡: {chat_history}

ì‚¬ìš©ì ì§ˆë¬¸: {query}

ìœ„ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”. 
ì»¨í…ìŠ¤íŠ¸ì— ê´€ë ¨ ì •ë³´ê°€ ì—†ë‹¤ë©´ ì¼ë°˜ì ì¸ ì§€ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”."""

if "llm_provider" not in st.session_state:
    st.session_state.llm_provider = "ChatGPT"

if "llama_model" not in st.session_state:
    st.session_state.llama_model = None

if "llama_tokenizer" not in st.session_state:
    st.session_state.llama_tokenizer = None

# âœ… ê°œì„ ëœ LLaMA ëª¨ë¸ ë¡œë”© í•¨ìˆ˜
@st.cache_resource
def load_optimized_llama_model(model_path, quantization_type="4bit", use_flash_attention=False):
    """ì–‘ìí™” ë° ìµœì í™”ê°€ ì ìš©ëœ LLaMA ëª¨ë¸ ë¡œë”©"""
    try:
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        
        # âœ… íŒ¨ë”© í† í° ì„¤ì • ê°œì„ 
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
            tokenizer.pad_token_id = tokenizer.eos_token_id
        
        # ì–‘ìí™” ì„¤ì •
        quantization_config = None
        if quantization_type == "4bit" and torch.cuda.is_available():
            quantization_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.float16,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4"
            )
            st.info("ğŸš€ 4bit ì–‘ìí™” ì ìš©")
            
        elif quantization_type == "8bit" and torch.cuda.is_available():
            quantization_config = BitsAndBytesConfig(
                load_in_8bit=True,
                bnb_8bit_compute_dtype=torch.float16
            )
            st.info("âš¡ 8bit ì–‘ìí™” ì ìš©")
        
        # ëª¨ë¸ ë¡œë“œ
        if torch.cuda.is_available() and quantization_config is not None:
            model = AutoModelForCausalLM.from_pretrained(
                model_path,
                quantization_config=quantization_config,
                device_map="auto",
                torch_dtype=torch.float16,
                trust_remote_code=True,
                low_cpu_mem_usage=True,
            )
        else:
            model = AutoModelForCausalLM.from_pretrained(
                model_path,
                torch_dtype=torch.float32,
                low_cpu_mem_usage=True,
                device_map={"": "cpu"}
            )
            st.info("ğŸ’» CPU ëª¨ë“œ")
        
        model.eval()
        
        # âœ… ì»´íŒŒì¼ ìµœì í™” (ì•ˆì „í•˜ê²Œ)
        if hasattr(torch, 'compile') and torch.cuda.is_available():
            try:
                model = torch.compile(model, mode="reduce-overhead")
                st.success("ğŸ”¥ PyTorch ì»´íŒŒì¼ ìµœì í™” ì ìš©")
            except Exception as e:
                st.warning(f"ì»´íŒŒì¼ ìµœì í™” ì‹¤íŒ¨: {str(e)}")
        
        return model, tokenizer
        
    except Exception as e:
        st.error(f"ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {str(e)}")
        return None, None

# API ì„¤ì • ì‚¬ì´ë“œë°”
with st.sidebar:
    st.title("API ì„¤ì •")
    
    st.session_state.llm_provider = st.selectbox(
        "LLM ì œê³µì ì„ íƒ",
        ["ChatGPT", "Open LLaMA (ìµœì í™”)"],
        index=0 if st.session_state.llm_provider == "ChatGPT" else 1
    )
    
    if st.session_state.llm_provider == "ChatGPT":
        api_key = st.text_input(
            "OpenAI API Key", 
            value="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",
            type="password"
        )
        if api_key:
            os.environ["OPENAI_API_KEY"] = api_key
    
    elif st.session_state.llm_provider == "Open LLaMA (ìµœì í™”)":
        st.markdown("### ğŸš€ ìµœì í™”ëœ Open LLaMA 3B ëª¨ë¸ ì„¤ì •")
        
        model_path = "./models"
        st.info(f"ëª¨ë¸ ê²½ë¡œ: {model_path}")
        
        col1, col2 = st.columns(2)
        with col1:
            quantization_type = st.selectbox(
                "ì–‘ìí™” íƒ€ì…",
                ["4bit", "8bit", "none"],
                index=0,
                help="4bit: ìµœëŒ€ ë©”ëª¨ë¦¬ ì ˆì•½"
            )
        
        with col2:
            use_flash_attention = st.checkbox(
                "Flash Attention 2", 
                value=False
            )
        
        device = st.selectbox(
            "ë””ë°”ì´ìŠ¤ ì„ íƒ",
            ["cuda", "cpu"],
            index=0 if torch.cuda.is_available() else 1
        )
        
        if quantization_type == "4bit":
            st.success("ğŸ¯ ì˜ˆìƒ ì„±ëŠ¥: ë©”ëª¨ë¦¬ 75%â†“, ì†ë„ 2-3ë°°â†‘")
        elif quantization_type == "8bit":
            st.info("âš¡ ì˜ˆìƒ ì„±ëŠ¥: ë©”ëª¨ë¦¬ 50%â†“, ì†ë„ 1.5-2ë°°â†‘")
        
        if device == "cpu":
            num_threads = st.slider("CPU ìŠ¤ë ˆë“œ ìˆ˜", 1, os.cpu_count(), os.cpu_count())
            st.warning("ğŸ’¡ CPUì—ì„œëŠ” ì–‘ìí™”ê°€ ì ìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        else:
            num_threads = None
        
        if st.button("ğŸš€ ìµœì í™”ëœ Open LLaMA ëª¨ë¸ ë¡œë“œ"):
            with st.spinner("ëª¨ë¸ ë¡œë”© ì¤‘..."):
                if num_threads:
                    torch.set_num_threads(num_threads)
                    
                model, tokenizer = load_optimized_llama_model(
                    model_path, 
                    quantization_type if device == "cuda" else "none",
                    use_flash_attention
                )
                
                if model and tokenizer:
                    st.session_state.llama_model = model
                    st.session_state.llama_tokenizer = tokenizer
                    st.success("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
                else:
                    st.error("ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨")
        
        if st.session_state.llama_model is not None:
            st.success("âœ“ ìµœì í™”ëœ ëª¨ë¸ ë¡œë“œë¨")
        else:
            st.info("ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

# íƒ­ êµ¬ì„±
tab1, tab2, tab3 = st.tabs(["Retriever", "Chatbot ì„¤ì •", "Rag_chatbot"])

# íƒ­ 1: Retriever ì„¤ì •
with tab1:
    st.header("Retriever ì„¤ì •")
    
    st.markdown("""
    **ì´ ë‹¨ê³„ì—ì„œëŠ” AIê°€ ì°¸ê³ í•  ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ê³  ê²€ìƒ‰ ì‹œìŠ¤í…œì„ ì„¤ì •í•©ë‹ˆë‹¤.**
    """)
    
    uploaded_file = st.file_uploader("í…ìŠ¤íŠ¸ ë˜ëŠ” PDF íŒŒì¼ ì—…ë¡œë“œ", type=["txt", "pdf"])
    
    col1, col2 = st.columns(2)
    
    with col1:
        splitter_type = st.selectbox(
            "í…ìŠ¤íŠ¸ ë¶„í•  ë°©ë²•",
            ["RecursiveCharacterTextSplitter", "CharacterTextSplitter"]
        )
        
        chunk_size = st.slider("Chunk Size", min_value=100, max_value=2000, value=200, step=50)
        chunk_overlap = st.slider("Chunk Overlap", min_value=0, max_value=500, value=0, step=50)
        
        embedding_model = st.selectbox(
            "ì„ë² ë”© ëª¨ë¸",
            ["text-embedding-3-small", "text-embedding-3-large", "text-embedding-ada-002"]
        )
    
    with col2:
        vectorstore_type = st.selectbox(
            "ë²¡í„° ì €ì¥ì†Œ ìœ í˜•",
            ["Chroma", "FAISS"]
        )
        
        search_type = st.selectbox(
            "ê²€ìƒ‰ ìœ í˜•",
            ["similarity", "similarity_score_threshold", "mmr"]
        )
        
        k_value = st.slider("ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ (k)", min_value=1, max_value=10, value=3)
        
        if search_type == "similarity_score_threshold":
            score_threshold = st.slider("Score Threshold", min_value=0.0, max_value=1.0, value=0.5, step=0.05)
        else:
            score_threshold = 0.5

def process_file():
    if uploaded_file is not None:
        file_extension = uploaded_file.name.split(".")[-1].lower()
        
        if file_extension == "txt":
            text_content = uploaded_file.getvalue().decode("utf-8")
        elif file_extension == "pdf":
            pdf_reader = PyPDF2.PdfReader(BytesIO(uploaded_file.read()))
            text_content = ""
            for page in pdf_reader.pages:
                text_content += page.extract_text() + "\n"
        else:
            st.error("ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤.")
            return False
        
        if not text_content.strip():
            st.error("íŒŒì¼ì— í…ìŠ¤íŠ¸ê°€ ì—†ê±°ë‚˜ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        if splitter_type == "RecursiveCharacterTextSplitter":
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=chunk_size, 
                chunk_overlap=chunk_overlap, 
                separators=["\n\n", "\n", "."]
            )
            paragraphs = text_splitter.split_text(text_content)
        else:
            text_splitter = CharacterTextSplitter(
                separator='\n',
                chunk_size=chunk_size, 
                chunk_overlap=chunk_overlap, 
                length_function=len
            )
            paragraphs = text_splitter.split_text(text_content)
        
        documents = [Document(page_content=para, metadata={"source": uploaded_file.name}) for para in paragraphs]
        st.session_state.documents = documents
        
        embedding = OpenAIEmbeddings(model=embedding_model)
        
        if vectorstore_type == "Chroma":
            vectordb = Chroma.from_documents(documents=documents, embedding=embedding)
        else:
            vectordb = FAISS.from_documents(documents=documents, embedding=embedding)
        
        st.session_state.vectordb = vectordb
        
        if search_type == "similarity":
            retriever = vectordb.as_retriever(search_kwargs={'k': k_value})
        elif search_type == "similarity_score_threshold":
            retriever = vectordb.as_retriever(
                search_type="similarity_score_threshold", 
                search_kwargs={'k': k_value, "score_threshold": score_threshold}
            )
        else:
            retriever = vectordb.as_retriever(
                search_type="mmr", 
                search_kwargs={'k': k_value}
            )
        
        st.session_state.retriever = retriever
        return True
    
    return False

with tab1:
    if st.button("ë°ì´í„° ì²˜ë¦¬"):
        with st.spinner("ë°ì´í„° ì²˜ë¦¬ ì¤‘..."):
            if process_file():
                st.success("ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ!")
                
                if st.session_state.documents:
                    random_doc = random.choice(st.session_state.documents)
                    st.subheader("ğŸ” ëœë¤ ì²­í‚¹ ë¬¸ì„œ ë¯¸ë¦¬ë³´ê¸°")
                    
                    file_ext = random_doc.metadata.get('source', 'Unknown').split('.')[-1].lower()
                    file_icon = "ğŸ“„" if file_ext == "pdf" else "ğŸ“"
                    
                    st.info(f"{file_icon} **ì¶œì²˜**: {random_doc.metadata.get('source', 'Unknown')}")
                    st.write(random_doc.page_content)
            else:
                st.error("íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")

# íƒ­ 2: Chatbot ì„¤ì •
with tab2:
    st.header("Chatbot ì„¤ì •")
    
    col1, col2 = st.columns(2)
    
    with col1:
        if st.session_state.llm_provider == "ChatGPT":
            llm_model = st.selectbox(
                "LLM ëª¨ë¸",
                ["gpt-4o-mini", "gpt-4", "gpt-4o"]
            )
        else:
            st.info("ìµœì í™”ëœ Open LLaMA 3B ëª¨ë¸ ì‚¬ìš©")
            llm_model = "optimized-open-llama-3b"
        
        temperature = st.slider("Temperature", min_value=0.0, max_value=1.0, value=0.7, step=0.1)
    
    with col2:
        max_turns = st.slider("ìµœëŒ€ ëŒ€í™” ê¸°ë¡ ìœ ì§€ ìˆ˜", min_value=1, max_value=10, value=3)
        
        if st.session_state.llm_provider == "Open LLaMA (ìµœì í™”)":
            max_new_tokens = st.slider(
                "ìµœëŒ€ ìƒì„± í† í° ìˆ˜", 
                min_value=32, 
                max_value=512, 
                value=128
            )
            top_p = st.slider("Top-p", min_value=0.1, max_value=1.0, value=0.9)
            use_cache = st.checkbox("KV ìºì‹œ ì‚¬ìš©", value=True)
            repetition_penalty = st.slider("ë°˜ë³µ ë°©ì§€", min_value=1.0, max_value=1.5, value=1.1, step=0.05)
    
    st.subheader("ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸")
    system_prompt = st.text_area(
        "AIì˜ ì—­í• ê³¼ ì‘ë‹µ ë°©ì‹ì„ ì„¤ì •í•©ë‹ˆë‹¤",
        st.session_state.system_prompt,
        height=200
    )
    st.session_state.system_prompt = system_prompt
    
    st.subheader("ìœ ì € í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿")
    user_prompt_template = st.text_area(
        "ê²€ìƒ‰ ê²°ê³¼ì™€ ì‚¬ìš©ì ì§ˆë¬¸ì„ ì¡°í•©í•˜ëŠ” ë°©ì‹ì„ ì„¤ì •í•©ë‹ˆë‹¤",
        st.session_state.user_prompt_template,
        height=150
    )
    st.session_state.user_prompt_template = user_prompt_template

# íƒ­ 3: ì±„íŒ… ì¸í„°í˜ì´ìŠ¤
with tab3:
    st.header("ğŸš€ ìµœì í™”ëœ RAG Chatbot")
    
    # ì„±ëŠ¥ ìƒíƒœ í‘œì‹œ
    if st.session_state.llm_provider == "Open LLaMA (ìµœì í™”)":
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            if st.session_state.llama_model is not None:
                st.success("âœ… ëª¨ë¸ ë¡œë“œë¨")
            else:
                st.error("âŒ ëª¨ë¸ ë¯¸ë¡œë“œ")
        
        with col2:
            if torch.cuda.is_available():
                st.success("ğŸš€ GPU í™œì„±í™”")
            else:
                st.info("ğŸ’» CPU ëª¨ë“œ")
        
        with col3:
            if torch.cuda.is_available() and st.session_state.llama_model is not None:
                memory_used = torch.cuda.memory_allocated(0) / 1e9
                memory_total = torch.cuda.get_device_properties(0).total_memory / 1e9
                memory_percent = (memory_used / memory_total) * 100
                if memory_percent < 50:
                    st.success(f"ğŸ’š ë©”ëª¨ë¦¬ {memory_percent:.0f}%")
                elif memory_percent < 80:
                    st.warning(f"ğŸŸ¡ ë©”ëª¨ë¦¬ {memory_percent:.0f}%")
                else:
                    st.error(f"ğŸ”´ ë©”ëª¨ë¦¬ {memory_percent:.0f}%")
            else:
                st.info("ğŸ’¾ ë©”ëª¨ë¦¬ ì •ë³´ ì—†ìŒ")
        
        with col4:
            if st.session_state.llama_model is not None:
                st.success("âš¡ ì–‘ìí™” ì ìš©")
            else:
                st.info("ğŸ”§ ì„¤ì • í•„ìš”")
    
    # ì±„íŒ… ë©”ì‹œì§€ í‘œì‹œ
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
    
    # âœ… ìƒˆë¡œìš´ RunnableSequence ë°©ì‹ì„ ì‚¬ìš©í•œ ì‘ë‹µ ìƒì„± í•¨ìˆ˜
    def generate_optimized_response(patient_message):
        if st.session_state.llm_provider == "ChatGPT" and not os.environ.get("OPENAI_API_KEY"):
            st.error("OpenAI API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            return None
        
        if st.session_state.llm_provider == "Open LLaMA (ìµœì í™”)" and st.session_state.llama_model is None:
            st.error("ë¨¼ì € ìµœì í™”ëœ Open LLaMA ëª¨ë¸ì„ ë¡œë“œí•´ì£¼ì„¸ìš”.")
            return None
        
        if st.session_state.retriever is None:
            st.error("ë¨¼ì € Retriever íƒ­ì—ì„œ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•´ì£¼ì„¸ìš”.")
            return None
        
        # ìµœëŒ€ ëŒ€í™” ê¸°ë¡ ì œí•œ
        MAX_TURNS = max_turns if 'max_turns' in locals() else 3
        st.session_state.memory.chat_memory.messages = st.session_state.memory.chat_memory.messages[-MAX_TURNS:]
        
        # âœ… ìƒˆë¡œìš´ retriever í˜¸ì¶œ ë°©ì‹
        try:
            retrieved_docs = st.session_state.retriever.invoke(patient_message)
        except:
            # fallback to old method
            retrieved_docs = st.session_state.retriever.get_relevant_documents(patient_message)
        
        context = "\n".join([doc.page_content for doc in retrieved_docs]) if retrieved_docs else "ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        # ëŒ€í™” ê¸°ë¡
        chat_history = "\n".join([
            f"ì‚¬ìš©ì: {msg.content}" if isinstance(msg, HumanMessage) else f"AI: {msg.content}"
            for msg in st.session_state.memory.chat_memory.messages
        ])
        
        # í”„ë¡¬í”„íŠ¸ í¬ë§·íŒ…
        formatted_prompt = st.session_state.user_prompt_template.format(
            chat_history=chat_history,
            context=context,
            query=patient_message
        )
        
        combined_input = f"{st.session_state.system_prompt}\n{formatted_prompt}"
        
        # LLM ì´ˆê¸°í™”
        if st.session_state.llm_provider == "ChatGPT":
            llm = ChatOpenAI(model_name=llm_model, temperature=temperature)
        else:
            try:
                llm = OptimizedDirectLLaMA(
                    model=st.session_state.llama_model,
                    tokenizer=st.session_state.llama_tokenizer,
                    max_new_tokens=max_new_tokens if 'max_new_tokens' in locals() else 128,
                    temperature=temperature,
                    top_p=top_p if 'top_p' in locals() else 0.9,
                    device="cuda" if torch.cuda.is_available() else "cpu",
                    use_cache=use_cache if 'use_cache' in locals() else True
                )
            except Exception as e:
                st.error(f"LLaMA ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
                return None
        
        # âœ… ìƒˆë¡œìš´ ë°©ì‹ìœ¼ë¡œ ì²´ì¸ ìƒì„± (deprecation ê²½ê³  í•´ê²°)
        prompt_template = PromptTemplate(
            input_variables=["input"],
            template="{input}"
        )
        
        # RunnableSequence ì‚¬ìš© (LLMChain ëŒ€ì‹ )
        chain = prompt_template | llm
        
        # ì‘ë‹µ ìƒì„± (ì‹œê°„ ì¸¡ì •)
        start_time = time.time()
        try:
            response = chain.invoke({"input": combined_input})
        except Exception as e:
            st.error(f"ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return None
        end_time = time.time()
        
        response_time = end_time - start_time
        
        # ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€
        st.session_state.memory.chat_memory.messages.append(HumanMessage(content=patient_message))
        st.session_state.memory.chat_memory.messages.append(AIMessage(content=response))
        
        return response, retrieved_docs, response_time
    
    # ì‚¬ìš©ì ì…ë ¥
    user_input = st.chat_input("ì§ˆë¬¸í•´ì£¼ì„¸ìš”!")
    if user_input:
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        st.session_state.messages.append({"role": "user", "content": user_input})
        
        # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ
        with st.chat_message("user"):
            st.markdown(user_input)
        
        # ì‘ë‹µ ìƒì„±
        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            full_response = ""
            
            # ì‘ë‹µ ìƒì„± ì¤‘ í‘œì‹œ
            if st.session_state.retriever is not None:
                with st.spinner("ğŸš€ ìµœì í™”ëœ AIê°€ ë‹µë³€ ìƒì„± ì¤‘..."):
                    response_data = generate_optimized_response(user_input)
                    
                    if response_data:
                        response, retrieved_docs, response_time = response_data
                        
                        # í† í° ë‹¨ìœ„ë¡œ ì‘ë‹µì„ ìƒì„±í•˜ëŠ” íš¨ê³¼
                        words = response.split()
                        for i, word in enumerate(words):
                            full_response += word + " "
                            time.sleep(0.01 if st.session_state.llm_provider == "Open LLaMA (ìµœì í™”)" else 0.02)
                            message_placeholder.markdown(full_response + "â–Œ")
                        
                        message_placeholder.markdown(response)
                        
                        # ì„±ëŠ¥ ì •ë³´ í‘œì‹œ
                        col1, col2, col3 = st.columns(3)
                        with col1:
                            st.caption(f"â±ï¸ ì‘ë‹µ ì‹œê°„: {response_time:.2f}ì´ˆ")
                        with col2:
                            if st.session_state.llm_provider == "Open LLaMA (ìµœì í™”)":
                                tokens_per_sec = len(response.split()) / response_time if response_time > 0 else 0
                                st.caption(f"ğŸš€ í† í°/ì´ˆ: {tokens_per_sec:.1f}")
                        with col3:
                            st.caption(f"ğŸ“„ ê²€ìƒ‰ëœ ë¬¸ì„œ: {len(retrieved_docs)}ê°œ")
                        
                        # ê²€ìƒ‰ëœ ë¬¸ì„œ í‘œì‹œ
                        with st.expander("ê²€ìƒ‰ëœ ê´€ë ¨ ë¬¸ì„œ"):
                            for i, doc in enumerate(retrieved_docs):
                                st.markdown(f"**ë¬¸ì„œ {i+1}**")
                                st.markdown(f"""
                                <div style="font-size: 1.1rem; line-height: 1.6; color: #333;">
                                {doc.page_content}
                                </div>
                                """, unsafe_allow_html=True)
                                st.markdown("---")
                        
                        # ì‘ë‹µ ì €ì¥
                        st.session_state.messages.append({"role": "assistant", "content": response})
            else:
                st.error("ë¨¼ì € Retriever íƒ­ì—ì„œ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•´ì£¼ì„¸ìš”.")
    
    # ëŒ€í™” ì´ˆê¸°í™” ë° ë©”ëª¨ë¦¬ ì •ë¦¬ ë²„íŠ¼
    col1, col2 = st.columns(2)
    with col1:
        if st.button("ğŸ—‘ï¸ ëŒ€í™” ì´ˆê¸°í™”"):
            st.session_state.messages = []
            st.session_state.memory.clear()
            st.rerun()
    
    with col2:
        if st.button("ğŸ§¹ ë©”ëª¨ë¦¬ ì •ë¦¬") and st.session_state.llm_provider == "Open LLaMA (ìµœì í™”)":
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                st.success("GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ!")
            else:
                st.info("CPU ëª¨ë“œì—ì„œëŠ” ë©”ëª¨ë¦¬ ì •ë¦¬ê°€ ë¶ˆí•„ìš”í•©ë‹ˆë‹¤.")
    
    # ìƒì„¸ GPU/CPU ìƒíƒœ í‘œì‹œ
    if st.session_state.llm_provider == "Open LLaMA (ìµœì í™”)":
        st.markdown("---")
        
        # ì„±ëŠ¥ ì •ë³´ í‘œì‹œ
        if st.session_state.llama_model is not None:
            with st.expander("ğŸ”§ ìƒì„¸ ì„±ëŠ¥ ì •ë³´"):
                col1, col2 = st.columns(2)
                
                with col1:
                    st.markdown("**ì‹œìŠ¤í…œ ì •ë³´**")
                    if torch.cuda.is_available():
                        gpu_name = torch.cuda.get_device_name(0)
                        st.text(f"GPU: {gpu_name}")
                        
                        # GPU ë©”ëª¨ë¦¬ ìƒì„¸ ì •ë³´
                        memory_allocated = torch.cuda.memory_allocated(0) / 1e9
                        memory_reserved = torch.cuda.memory_reserved(0) / 1e9
                        memory_total = torch.cuda.get_device_properties(0).total_memory / 1e9
                        
                        st.text(f"í• ë‹¹ëœ ë©”ëª¨ë¦¬: {memory_allocated:.2f}GB")
                        st.text(f"ì˜ˆì•½ëœ ë©”ëª¨ë¦¬: {memory_reserved:.2f}GB")
                        st.text(f"ì „ì²´ ë©”ëª¨ë¦¬: {memory_total:.2f}GB")
                        
                        # ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  í”„ë¡œê·¸ë ˆìŠ¤ ë°”
                        memory_usage = memory_allocated / memory_total
                        st.progress(memory_usage, text=f"ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ : {memory_usage*100:.1f}%")
                    else:
                        st.text("CPU ëª¨ë“œ")
                        st.text(f"CPU ì½”ì–´: {os.cpu_count()}ê°œ")
                        st.text(f"PyTorch ìŠ¤ë ˆë“œ: {torch.get_num_threads()}ê°œ")
                
                with col2:
                    st.markdown("**ëª¨ë¸ ì •ë³´**")
                    if st.session_state.llama_model is not None:
                        total_params = sum(p.numel() for p in st.session_state.llama_model.parameters())
                        trainable_params = sum(p.numel() for p in st.session_state.llama_model.parameters() if p.requires_grad)
                        
                        st.text(f"ì „ì²´ íŒŒë¼ë¯¸í„°: {total_params/1e9:.2f}B")
                        st.text(f"í•™ìŠµ íŒŒë¼ë¯¸í„°: {trainable_params/1e9:.2f}B")
                        
                        # ì–‘ìí™” ìƒíƒœ ì¶”ì •
                        if total_params < 2e9:
                            st.text("ìƒíƒœ: ì–‘ìí™” ì ìš©ë¨ (ì¶”ì •)")
                        else:
                            st.text("ìƒíƒœ: ì „ì²´ ì •ë°€ë„")
                            
                        # ëª¨ë¸ ë°ì´í„° íƒ€ì…
                        sample_param = next(st.session_state.llama_model.parameters())
                        st.text(f"ë°ì´í„° íƒ€ì…: {sample_param.dtype}")
        
        # ìµœì í™” íŒ í‘œì‹œ
        st.markdown("---")
        st.markdown("### ğŸš€ ì„±ëŠ¥ ìµœì í™” íŒ")
        
        tips_col1, tips_col2 = st.columns(2)
        
        with tips_col1:
            st.markdown("""
            **ì†ë„ í–¥ìƒì„ ìœ„í•´:**
            - 4bit ì–‘ìí™” ì‚¬ìš©
            - í† í° ìˆ˜ë¥¼ 32-128ë¡œ ì œí•œ
            - Temperature 0.1-0.7 ì‚¬ìš©
            - KV ìºì‹œ í™œì„±í™”
            """)
        
        with tips_col2:
            st.markdown("""
            **ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´:**
            - ëŒ€í™” ê¸°ë¡ ì œí•œ (3-5í„´)
            - ì •ê¸°ì ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì •ë¦¬
            - ê¸´ ë¬¸ì„œëŠ” ì²­í¬ í¬ê¸° ì¤„ì´ê¸°
            - ë¶ˆí•„ìš”ì‹œ ëª¨ë¸ ì–¸ë¡œë“œ
            """)
        
        # Open LLaMA ë¼ì´ì„¼ìŠ¤ í‘œì‹œ
        st.markdown("---")
        st.markdown("""
        <div style="text-align: center; color: #666; font-size: 0.9rem; margin-top: 20px;">
            <p><strong>âš¡ Open LLaMA model with Quantization Optimization</strong></p>
            <p>Open LLaMA is licensed under Apache License 2.0 | Released by OpenLM Research</p>
            <p>Quantization powered by BitsAndBytes | ğŸš€ Performance Enhanced</p>
        </div>
        """, unsafe_allow_html=True)

# âœ… ì¶”ê°€ ì„¤ì¹˜ ì•ˆë‚´
if st.session_state.llm_provider == "Open LLaMA (ìµœì í™”)" and st.session_state.llama_model is None:
    with st.expander("ğŸ“¦ í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì•ˆë‚´"):
        st.markdown("""
        **ì–‘ìí™”ë¥¼ ìœ„í•´ ë‹¤ìŒ íŒ¨í‚¤ì§€ë“¤ì´ í•„ìš”í•©ë‹ˆë‹¤:**
        
        ```bash
        # í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜
        pip install bitsandbytes accelerate
        pip install --upgrade transformers torch
        
        # charset_normalizer ê²½ê³  í•´ê²°
        pip install charset_normalizer
        
        # ì„ íƒì‚¬í•­: Flash Attention (ì„±ëŠ¥ í–¥ìƒ)
        pip install flash-attn --no-build-isolation
        ```
        
        **ì£¼ì˜ì‚¬í•­:**
        - `bitsandbytes`ëŠ” CUDAê°€ ì„¤ì¹˜ëœ í™˜ê²½ì—ì„œë§Œ ì •ìƒ ì‘ë™í•©ë‹ˆë‹¤
        - CPUì—ì„œëŠ” ì–‘ìí™”ê°€ ì ìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤
        - Windowsì—ì„œ `bitsandbytes` ì„¤ì¹˜ ì‹œ ì¶”ê°€ ì„¤ì •ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
        """)

# âœ… ë¬¸ì œ í•´ê²° ê°€ì´ë“œ
with st.expander("ğŸ”§ ë¬¸ì œ í•´ê²° ê°€ì´ë“œ"):
    st.markdown("""
    **ìì£¼ ë°œìƒí•˜ëŠ” ë¬¸ì œë“¤ê³¼ í•´ê²° ë°©ë²•:**
    
    1. **"RequestsDependencyWarning" ê²½ê³ **
       ```bash
       pip install charset_normalizer
       ```
    
    2. **"LangChainDeprecationWarning" ê²½ê³ **
       - ì´ëŠ” ì •ìƒì ì¸ ê²½ê³ ì…ë‹ˆë‹¤. ì½”ë“œê°€ ìµœì‹  ë°©ì‹ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.
    
    3. **"attention mask is not set" ê²½ê³ **
       - ì´ë¯¸ ì½”ë“œì—ì„œ í•´ê²°ë˜ì—ˆìŠµë‹ˆë‹¤. attention_maskë¥¼ ìë™ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤.
    
    4. **GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ì˜¤ë¥˜**
       - 4bit ì–‘ìí™” ì‚¬ìš©
       - ìµœëŒ€ í† í° ìˆ˜ë¥¼ 32-128ë¡œ ì œí•œ
       - ëŒ€í™” ê¸°ë¡ì„ 3-5í„´ìœ¼ë¡œ ì œí•œ
    
    5. **ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨**
       - ëª¨ë¸ ê²½ë¡œ í™•ì¸: `./models` ë””ë ‰í† ë¦¬ì— ëª¨ë¸ íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸
       - GPU ë©”ëª¨ë¦¬ í™•ì¸: ìµœì†Œ 4GB ì´ìƒ í•„ìš”
       - ê¶Œí•œ ë¬¸ì œ: ëª¨ë¸ íŒŒì¼ì— ì½ê¸° ê¶Œí•œì´ ìˆëŠ”ì§€ í™•ì¸
    
    6. **PyTorch ê´€ë ¨ ì˜¤ë¥˜**
       - ìµœì‹  ë²„ì „ìœ¼ë¡œ ì—…ë°ì´íŠ¸: `pip install --upgrade torch torchvision torchaudio`
       - CUDA ë²„ì „ í™•ì¸: `torch.cuda.is_available()`
    """)

st.markdown("---")
st.markdown("""
<div style="text-align: center; color: #666; margin-top: 20px;">
    <p>ğŸŒ¸ <strong>Tokbom RAG Chatbot</strong> - ë¬¸ì„œ ê¸°ë°˜ AI ìƒë‹´ ì‹œìŠ¤í…œ</p>
    <p>ìµœì í™”ëœ ì„±ëŠ¥ê³¼ ì•ˆì •ì„±ì„ ì œê³µí•©ë‹ˆë‹¤</p>
</div>
""", unsafe_allow_html=True)
